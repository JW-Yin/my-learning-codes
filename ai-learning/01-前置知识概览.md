## Transformer 多头注意力（MHA）中 Q、K、V 
Transformer 是一种 “编码器 - 解码器” 架构的模型，其核心运行逻辑依赖注意力机制的 Q-K-V 交互：
- 输入先通过 Embedding 向量化并加入位置信息，
- 编码器通过自注意力（Q-K-V 来自同一输入）捕捉输入序列的全局依赖，
- 解码器通过自注意力（关注已生成内容）和交叉注意力（Q 来自解码器，K/V 来自编码器）生成目标序列，
- 最终通过输出层将向量转回自然语言。

## Embedding（向量嵌入）
- 本质：将文本 / 图像等非结构化数据转化为计算机可理解的高维向量，向量空间中语义越相似的文本，向量距离越近

- 两种 Embedding 的区别
    - Transformer 中的 Embedding:  
    是模型输入层的一部分（如词嵌入、位置嵌入），目的是让模型理解文本的语义和位置信息，属于模型内部的表示
    - RAG 中的 Embedding:  
    用于将外部文档 / 用户查询转化为向量，目的是做相似度检索（匹配相关文档），属于模型外部的检索工具

- 为什么 RAG 需要 Embedding？  
因为大模型自身知识有限且可能过时，Embedding 能把用户问题和私有文档转成向量，通过向量相似度找到最相关的文档，再让模型基于这些文档生成答案，避免幻觉、提升精准度。
- Reranker 的作用是什么？  
Embedding 召回的文档是基于向量相似度的粗排，Reranker 会对这些文档做细粒度语义重排，进一步提升文档的相关性，是 RAG 流程中 “召回→排序” 的关键一步。

## Milvus - 向量数据库
- 本质：专门存储向量数据的数据库，支持高效的近似最近邻搜索（ANN），解决传统数据库无法快速计算高维向量相似度的问题

- 为什么不用传统数据库（如 MySQL）存向量？  
传统数据库是为结构化数据设计的，无法高效计算高维向量的相似度，而向量数据库通过；向量索引算法能把检索时间从 O (n) 降到 O (log n)。

- 常用向量索引算法有哪些？  
    - HNSW：层次化可导航小世界图，适合高维向量的快速近邻搜索，是目前最主流的索引算法。
    - IVF：倒排文件，将向量聚类，检索时先找相似的簇再在簇内搜索，适合大规模数据。
    - PQ：乘积量化，将高维向量拆成低维子向量压缩存储，适合内存有限的场景。

- 混合检索是什么？  
结合向量相似度检索和关键词检索，比如在保险场景中，既用向量匹配 “理赔申请” 的语义，又用关键词过滤 “2025 年” 的时间范围，提升检索精准度。

- Milvus如何导入向量数据？  
先创建 Collection（集合），定义向量维度和索引类型，再插入向量和对应的元数据（如文档 ID、文本片段）。


## Neo4j - 图数据库
存储实体间的关系，适合做关系型推理。在检索增强生成（RAG）系统中，Neo4j 特别适合存储结构化的知识与关联信息，弥补纯向量库的不足（文档间的引用关系、实体间关系、对话轮次的依赖关系、用户意图的演变链路、等等各种依赖关系，能用于复杂推理和关联检索，让 RAG 回答更具深度和专业性）
- 核心概念：
    - 节点（Node）：代表实体，如 “文档”“人物”“概念”“产品”，每个节点有唯一 ID 和标签
    - 关系（Relationship）：代表实体间的连接，如 “文档引用文档”“人物撰写文档”“概念属于领域”，关系有方向和类型
    - 属性（Property）：节点或关系的特征

## MongoDB - NoSQL 数据库
存储非结构化 / 半结构化数据，PostpreSQL几乎可完全替代掉MonogoDB

## 检索增强生成（Retrieval-Augmented Generation，RAG）
本质：让 AI 生成的答案更精准、可溯源、无幻觉，同时能动态接入私有数据和实时信息（减少幻觉/知识实时更新/可溯源）
- 为什么需要 RAG？  
    大模型（如 GPT、Qwen）的训练数据是固定的（比如截止到某个时间点），且无法访问你的私有数据，这会导致三大问题：

    - 知识过时：无法回答训练数据之后的新信息（如 “2025 年新推出的医疗险理赔规则”）。
    - 容易 “幻觉”：会编造看似合理但错误的内容（比如编造不存在的保险条款）。
    - 无法访问私有数据：不能直接使用你的内部知识库（如公司保险手册、客户保单数据）。

    RAG 的本质就是给大模型 “外挂一个可靠的知识库”，让它生成答案前先去知识库找依据，相当于 “带着参考资料写作文”，从根源上解决这些问题。
- RAG 分为离线准备和在线推理两个阶段
    - 离线准备（提前处理知识库）
        - 把私有文档用 LangChain 的 Loader 加载进来
        - 用文本分割器把长文档拆成语义完整的小片段，避免上下文断裂 
        - 用 Embedding 模型把每个文本片段转成高维向量
        - 把向量和对应的文本片段存入向量数据库，建立索引
    - 在线推理（用户提问时动态检索）
        - 把用户的问题转成向量
        - 在向量数据库中检索和问题向量最相似的文本片段
        - 把检索到的文档片段和用户问题一起发给大模型，让模型基于这些文档生成精准答案

## LangChain
核心封装了大模型应用开发的通用核心操作，能一站式实现 LLM 调用、自动维护对话上下文、快速搭建 RAG 检索增强、调用外部工具 / 数据库等能力，无需开发者手动从零实现这些逻辑，是 RAG 开发的 “效率神器”，它封装了 RAG 全流程的所有组件：

- 文档加载器（Loader）→ 加载保险手册
- 文本分割器（Splitter）→ 拆分文档片段
- Embedding 模型集成 → 生成向量
- 向量数据库适配 → 存储向量
- 检索链（RetrievalQA）→ 串联 “检索→生成” 全流程

## 普通模型的处理流程：
```bash
人类提问→ 
分词器拆成 Token → 
大模型内部 Embedding → 
Transformer 解码器处理 → 
生成回答
```

## RAG增强后的模型处理流程：
```bash
人类提问 → 
LangChain 接收提问 → 
调用 RAG-Embedding 生成向量 → 
向量数据库（结合 RAG 技巧）检索相关文档 → 
LangChain 自动拼接 Prompt → 
分词/Embedding → 
解码器生成回答
```

##  Function Calling（函数调用 / 工具调用）
大模型本身不直接执行外部代码 / 请求，它只输出「标准化结构化调用指令」；由你的宿主程序解析指令、真正调用外部工具，再把结果塞回对话，让模型生成最终回答
- 本质：模型不是 “学会调用函数”，而是学会输出符合 Schema 的 JSON，本质还是语言模型的生成能力。即：Function Calling = 模型输出结构化调用指令 + 外部系统执行工具 + 结果回填生成回答
- 使用流程
    - 预先定义工具清单（给模型看的 “接口文档”）
        - 用 JSON Schema 告诉模型：有哪些工具、功能、参数格式、必填项
    - 用户发起自然语言查询
    - 大模型推理决策
        - 模型理解意图 → 
        - 判断必须调用外部工具 → 
        - 不直接回答，而是输出结构化调用指令
    - 宿主程序解析并真正调用外部工具
    - 工具结果回填 → 模型生成最终回答

## RAG查询分解（Query Decomposition）
将复杂、多维度的用户问题拆解为多个独立、可检索的子问题，分别检索后再整合答案。可提升检索覆盖度，使答案更全面、逻辑更清晰

## RAG重排序（Reranking）
向量检索（Bi-Encoder）速度快但精度有限，易返回 “看似相关” 的无效内容。此时可以进行Reranking，在初筛（向量检索）后，用更精准的模型对候选文档二次打分，筛选出最相关的 Top-K 片段。可显著提升检索精度，是 RAG 优化中 ROI 最高的技术之一

## RAG-HyDE（假设文档生成，Hypothetical Document Embeddings）
用户查询通常简短、语义稀疏，直接检索易遗漏相关文档。可以先让大模型基于用户查询生成一份 “假设的理想答案”（虚拟文档），再用该文档的向量去检索真实知识库，可丰富查询语义，提升检索召回率

- 通常流程
    - 生成假设文档：LLM 根据查询生成一段包含专业术语、逻辑细节的 “理想答案”（允许包含虚构细节）
    - 向量检索：用假设文档的嵌入向量在知识库中检索相似的真实文档
    - 生成最终答案：基于检索到的真实文档生成回答