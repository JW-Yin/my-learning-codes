## Transformer 多头注意力（MHA）中 Q、K、V 
Transformer 是一种 “编码器 - 解码器” 架构的模型，其核心运行逻辑依赖注意力机制的 Q-K-V 交互：输入先通过 Embedding 向量化并加入位置信息，编码器通过自注意力（Q-K-V 来自同一输入）捕捉输入序列的全局依赖，解码器通过自注意力（关注已生成内容）和交叉注意力（Q 来自解码器，K/V 来自编码器）生成目标序列，最终通过输出层将向量转回自然语言。

## Embedding（向量嵌入）
- 本质：将文本 / 图像等非结构化数据转化为计算机可理解的高维向量，向量空间中语义越相似的文本，向量距离越近

- 两种 Embedding 的区别
    - Transformer 中的 Embedding:  
    是模型输入层的一部分（如词嵌入、位置嵌入），目的是让模型理解文本的语义和位置信息，属于模型内部的表示
    - RAG 中的 Embedding:  
    用于将外部文档 / 用户查询转化为向量，目的是做相似度检索（匹配相关文档），属于模型外部的检索工具

- 为什么 RAG 需要 Embedding？  
因为大模型自身知识有限且可能过时，Embedding 能把用户问题和私有文档转成向量，通过向量相似度找到最相关的文档，再让模型基于这些文档生成答案，避免幻觉、提升精准度。
- Reranker 的作用是什么？  
Embedding 召回的文档是基于向量相似度的粗排，Reranker 会对这些文档做细粒度语义重排，进一步提升文档的相关性，是 RAG 流程中 “召回→排序” 的关键一步。

## Milvus - 向量数据库
- 本质：专门存储向量数据的数据库，支持高效的近似最近邻搜索（ANN），解决传统数据库无法快速计算高维向量相似度的问题

- 为什么不用传统数据库（如 MySQL）存向量？  
传统数据库是为结构化数据设计的，无法高效计算高维向量的相似度，而向量数据库通过；向量索引算法能把检索时间从 O (n) 降到 O (log n)。

- 常用向量索引算法有哪些？  
    - HNSW：层次化可导航小世界图，适合高维向量的快速近邻搜索，是目前最主流的索引算法。
    - IVF：倒排文件，将向量聚类，检索时先找相似的簇再在簇内搜索，适合大规模数据。
    - PQ：乘积量化，将高维向量拆成低维子向量压缩存储，适合内存有限的场景。

- 混合检索是什么？  
结合向量相似度检索和关键词检索，比如在保险场景中，既用向量匹配 “理赔申请” 的语义，又用关键词过滤 “2025 年” 的时间范围，提升检索精准度。

- Milvus如何导入向量数据？  
先创建 Collection（集合），定义向量维度和索引类型，再插入向量和对应的元数据（如文档 ID、文本片段）。


## Neo4j - 图数据库
存储实体间的关系，适合做关系型推理

## MongoDB - NoSQL 数据库
存储非结构化 / 半结构化数据

## 检索增强生成（Retrieval-Augmented Generation，RAG）
本质：让 AI 生成的答案更精准、可溯源、无幻觉，同时能动态接入私有数据和实时信息（减少幻觉/知识实时更新/可溯源）
- 为什么需要 RAG？  
    大模型（如 GPT、Qwen）的训练数据是固定的（比如截止到某个时间点），且无法访问你的私有数据，这会导致三大问题：

    - 知识过时：无法回答训练数据之后的新信息（如 “2025 年新推出的医疗险理赔规则”）。
    - 容易 “幻觉”：会编造看似合理但错误的内容（比如编造不存在的保险条款）。
    - 无法访问私有数据：不能直接使用你的内部知识库（如公司保险手册、客户保单数据）。

    RAG 的本质就是给大模型 “外挂一个可靠的知识库”，让它生成答案前先去知识库找依据，相当于 “带着参考资料写作文”，从根源上解决这些问题。
- RAG 分为离线准备和在线推理两个阶段
    - 离线准备（提前处理知识库）
        - 把私有文档用 LangChain 的 Loader 加载进来
        - 用文本分割器把长文档拆成语义完整的小片段，避免上下文断裂 
        - 用 Embedding 模型把每个文本片段转成高维向量
        - 把向量和对应的文本片段存入向量数据库，建立索引
    - 在线推理（用户提问时动态检索）
        - 把用户的问题转成向量
        - 在向量数据库中检索和问题向量最相似的文本片段
        - 把检索到的文档片段和用户问题一起发给大模型，让模型基于这些文档生成精准答案

## LangChain
核心封装了大模型应用开发的通用核心操作，能一站式实现 LLM 调用、自动维护对话上下文、快速搭建 RAG 检索增强、调用外部工具 / 数据库等能力，无需开发者手动从零实现这些逻辑，是 RAG 开发的 “效率神器”，它封装了 RAG 全流程的所有组件：

- 文档加载器（Loader）→ 加载保险手册
- 文本分割器（Splitter）→ 拆分文档片段
- Embedding 模型集成 → 生成向量
- 向量数据库适配 → 存储向量
- 检索链（RetrievalQA）→ 串联 “检索→生成” 全流程

## 普通模型的处理流程：
```bash
人类提问→ 
分词器拆成 Token → 
大模型内部 Embedding → 
Transformer 解码器处理 → 
生成回答
```

## RAG增强后的模型处理流程：
```bash
人类提问 → 
LangChain 接收提问 → 
调用 RAG-Embedding 生成向量 → 
向量数据库（结合 RAG 技巧）检索相关文档 → 
LangChain 自动拼接 Prompt → 
分词/Embedding → 
解码器生成回答
```